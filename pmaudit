#!/usr/bin/env python3
"""
Run a command with file auditing or query the resulting audit data.

When run with a "--" separator %(prog)s will invoke the command to the
right of the separator and record subsequent file accesses to the file
named by the --json option in .json format. When run without "--" it reads the
audit file and dumps results (things like prereqs, targets, etc) from it.
The audit file defaults to "%(prog)s.json".

Audited files are divided into 4 categories: prerequisites, intermediate
targets, final targets, and unused:

  - A prerequisite is a file which was opened only for read; commonly
  this will be a source file originally checked out from SCM.

  - An intermediate target is one that was opened for both write and read
  (in that order). The most common example would be a .o file created
  by the compiler and later read by the linker.

  - A final target is a file which was opened only for write.
  Typically it's one of the final, deliverable build artifacts, the
  thing you set out to build. An example would be an executable file
  created by the linker.

  - An unused file, naturally, is one not opened at all during the build.

HOW IT WORKS:

This is the simplest form of file access auditing and could almost be done
by hand without scripting. It relies only on standard Unix file access
(atime) and modification (mtime) semantics. Before running the audited
command, the timestamps of all existing files in the audited directory
tree are recorded.  Then, as soon as the command finishes, all files in
the directory are rechecked.  If a file existed before the audit and
neither atime nor mtime has moved after, it was clearly unused.  If only
the atime has advanced it's a prereq. If it came into existence during the
audit it's a target, and if atime > mtime it's an intermediate target.
Etc.

A nice benefit of this simplicity is that there's practically no
performance cost. Most build-auditing techniques rely on a special
filesystem or ptrace which can slow things down measurably. This approach
has no effect on the running build; the only cost is that of traversing
and stat-ing every file in the audited directory before and after.

It won't work unless the filesystem enables atime-updating behavior, which
is the traditional default but is often disabled by NFS servers for
performance reasons. Thus it must be used either in local disk or an NFS
mount without the "noatime" option. An error is given if atimes don't seem
to be updated in the audited directory's filesystem.

Also, due to the introduction of "relatime" behavior on Linux it's become
necessary to artificially set each pre-existing file's atime prior to its
mtime during prior-state collection.  Read up on relatime for background.
Looking forward, the existence of relatime may make the use of noatime in
NFS less common.

If the underlying filesystem doesn't support sufficient timestamp
precision, results may be iffy. Clearly, when atimes and mtimes are
recorded only as seconds all files touched within the same second will
have the same timestamp and confusion may occur. Thus this is best used on
a modern Linux filesystem which records nanoseconds or similar.  It's not
that nanosecond precision per se is required, just that the more precision
the better.

As with any file-auditing technology this might expose underlying
weaknesses in the build model. I.e. if a file shows up in the wrong
category it may reveal a race condition.

EXAMPLES:

Run an audited make command and leave the results in %(prog)s.json:

    make -C subdir clean; %(prog)s --json %(prog)s.json -- make -C subdir ...

Dump the prerequisite set derived by the audit above:

    %(prog)s -P %(prog)s.json

Dump the discovered targets, both intermediate and final:

    %(prog)s -T %(prog)s.json
"""

###############################################################################
# Copyright (C) 2010-2018 David Boyce
#
# This program is free software; you can redistribute it and/or modify it under
# the terms of the GNU General Public License as published by the Free Software
# Foundation; either version 3 of the License, or (at your option) any later
# version.
#
# This program is distributed in the hope that it will be useful, but WITHOUT
# ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
# FOR A PARTICULAR PURPOSE. See the GNU General Public License for more detail.
#
# You may have received a copy of the GNU General Public License along with
# this program.  If not, see <http://www.gnu.org/licenses/>.
###############################################################################

import argparse
import collections
import datetime
import fcntl
import json
import logging
import os
import socket
import stat
import subprocess
import sys
import time

PROG = os.path.basename(__file__)

FMT1 = '%.07f'  # Format for one time value
FMTN = ['-2', '-1']
FMTU = ['0', '0']

HOSTNAME = 'HOSTNAME'
BASE = 'BASE'
CMD = 'CMD'
START = 'START'
PRIOR_COUNT = 'PRIOR_COUNT'
AFTER_COUNT = 'AFTER_COUNT'
PREREQS = 'PREREQS'
INTERMEDIATES = 'INTERMEDIATES'
FINALS = 'FINALS'
UNUSED = 'UNUSED'
DB = 'DB'

# I don't think the mtime - atime delta matters except
# that it must be >1 second to avoid roundoff errors.
DELTA = 24 * 60 * 60


# **NOTE** this is a copy from Python2 os.path.walk() since that function
# has been removed in Python3 in favor of os.walk(). Unfortunately os.walk
# tends to update symlink atimes so we can't use it.
def os_path_walk(top, func, arg):
    """Directory tree walk with callback function.

    For each directory in the directory tree rooted at top (including top
    itself, but excluding '.' and '..'), call func(arg, dirname, fnames).
    dirname is the name of the directory, and fnames a list of the names of
    the files and subdirectories in dirname (excluding '.' and '..').  func
    may modify the fnames list in-place (e.g. via del or slice assignment),
    and walk will only recurse into the subdirectories whose names remain in
    fnames; this can be used to implement a filter, or to impose a specific
    order of visiting.  No semantics are defined for, or required of, arg,
    beyond that arg is always passed to func.  It can be used, e.g., to pass
    a filename pattern, or a mutable object designed to accumulate
    statistics.  Passing None for arg is common."""

    try:
        names = os.listdir(top)
    except OSError:
        return
    func(arg, top, names)
    for name in names:
        name = os.path.join(top, name)
        try:
            st = os.lstat(name)
        except OSError:
            continue
        if stat.S_ISDIR(st.st_mode):
            os_path_walk(name, func, arg)


def die(message):
    """Report failure message and exit program with failure."""
    sys.stderr.write(message + '\n')
    sys.exit(1)


def mkdir_p(new_dir):
    """Make a directory sequence if it doesn't exist already."""
    if not new_dir:
        return  # Presume empty/current directory exists.
    # This raises an exception if can't create (e.g., if it's a file):
    os.makedirs(new_dir, exist_ok=True)


class PMAudit(object):

    """Track files used (prereqs) and generated (targets)."""

    def __init__(self, watchdirs, exclude=None):
        self.watchdirs = watchdirs
        self.exclude = set(['.git', '.svn'])
        if exclude:
            self.exclude |= exclude
        self.prereqs = collections.OrderedDict()
        self.intermediates = collections.OrderedDict()
        self.finals = collections.OrderedDict()
        self.unused = collections.OrderedDict()
        self.reftime = None
        self.prior = {}

    def start(self, flush_host=None, keep_going=False):
        """
        Start the build audit.

        There are some builds which touch their prerequisites,
        causing them to look like targets. To protect against
        that we use the belt-and-suspenders approach of checking
        against a list of files which predated the build.

        Also, due to the introduction of the Linux 'relatime' option
        it's necessary to prime the atime pump before starting. This
        is done by making all atimes a bit earlier than their mtimes.
        """

        mkflags = os.getenv('MAKEFLAGS')
        if mkflags and ' -j' in mkflags:
            logging.error('not supported in -j mode')
            sys.exit(2)

        for watchdir in self.watchdirs:
            # Figure out how atime updates are handled in this filesystem.
            ref_fname = os.path.join(watchdir, '.audit.%d.tmp' % os.getpid())
            with open(ref_fname, 'w') as f:
                f.write('data\n')
                ostats = os.fstat(f.fileno())
            os.utime(ref_fname, (ostats.st_mtime - DELTA, ostats.st_mtime))
            with open(ref_fname) as f:
                f.read()
            nstats = os.stat(ref_fname)
            needflush = nstats.st_atime < nstats.st_mtime
            if needflush:
                nfs_flush({ref_fname: (nstats.st_atime, nstats.st_mtime,
                                       True)}, host=flush_host)
                with open(ref_fname) as f:
                    f.read()
                nstats = os.stat(ref_fname)
                apath = os.path.dirname(os.path.abspath(ref_fname))
                if nstats.st_atime < nstats.st_mtime:
                    msg = 'atimes not updated in %s' % apath
                    if not keep_going:
                        logging.error(msg)
                        sys.exit(2)
                    logging.warning(msg)
                else:
                    logging.info('NFS flush required in %s', apath)
            os.remove(ref_fname)

            for parent, dnames, fnames in os.walk(watchdir):
                dnames[:] = (dn for dn in dnames if dn not in self.exclude)
                for fname in fnames:
                    if fname in self.exclude:
                        continue
                    path = os.path.relpath(os.path.join(parent, fname))
                    if os.path.islink(path):
                        continue
                    # Modern Linux won't update atime unless it's
                    # older than mtime (the "relatime" feature).
                    stats = os.stat(path)
                    atime, mtime = (stats.st_atime, stats.st_mtime)
                    if atime >= mtime:
                        atime = mtime - DELTA
                        os.utime(path, (atime, mtime))
                    self.prior[path] = (atime, mtime, needflush)

        nfs_flush(self.prior, host=flush_host)

        self.reftime = time.time()

    def finish(self, cmd=None):
        """End the audit, return the result."""

        # Record the set of surviving files with their times,
        # dividing them into the standard categories.
        # For each recorded file 4 timestamps are kept:
        # "pre-atime,pre-mtime,post-atime,post-mtime".
        # This data isn't needed once files have been categorized
        # but may be helpful in analysis or debugging.
        # Note: we can't use os.walk() for this because it has a
        # way of updating symlink atimes.
        prereqs, intermediates, finals, unused = {}, {}, {}, {}

        def visit(prunedirs, parent, fnames):
            """Callback function for os_path_walk() to categorize files."""
            for prunedir in prunedirs:
                if parent.startswith(prunedir):
                    return
            if os.path.basename(parent) in self.exclude:
                prunedirs.add(parent + os.sep)
                return
            for fname in fnames:
                if fname in self.exclude:
                    continue
                path = os.path.relpath(os.path.join(parent, fname))
                if os.path.isdir(path):
                    continue
                stats = os.lstat(path)
                atime, mtime = stats.st_atime, stats.st_mtime
                pstate = self.prior.get(path)
                if pstate:
                    if atime > pstate[0]:
                        val = [FMT1 % pstate[0], FMT1 % pstate[1],
                               FMT1 % atime, FMT1 % mtime]
                        if mtime > pstate[1]:
                            if mtime > atime:
                                finals[path] = val
                                msg = 'pre-existing file is final'
                            else:
                                intermediates[path] = val
                                msg = 'pre-existing file is target'
                            logging.info('%s: %s', msg, path)
                        else:
                            prereqs[path] = val
                    elif mtime > pstate[1]:
                        val = [FMT1 % pstate[0], FMT1 % pstate[1],
                               FMT1 % atime, FMT1 % mtime]
                        finals[path] = val
                        logging.info('pre-existing file modified: %s', path)
                    else:
                        val = [FMT1 % pstate[0], FMT1 % pstate[1]] + FMTU
                        unused[path] = val
                        continue
                else:
                    val = FMTN + [FMT1 % atime, FMT1 % mtime]
                    if mtime < atime:
                        intermediates[path] = val
                    else:
                        finals[path] = val

        for watchdir in self.watchdirs:
            os_path_walk(watchdir, visit, set())  # pylint: disable=no-member

        # Sort the data just derived. Not needed but helps readability.
        for k in sorted(prereqs):
            self.prereqs[k] = prereqs[k]
        for k in sorted(intermediates):
            self.intermediates[k] = intermediates[k]
        for k in sorted(finals):
            self.finals[k] = finals[k]
        for k in sorted(unused):
            self.unused[k] = unused[k]

        # Build up and return a serializable database.
        root = collections.OrderedDict()
        root[HOSTNAME] = socket.gethostname()
        root[BASE] = os.getcwd()
        root[CMD] = str(cmd)
        dt = datetime.datetime.utcfromtimestamp(self.reftime)
        refstr = dt.strftime('%Y-%m-%dT%H:%M:%S.%fZ')
        root[START] = [refstr, FMT1 % self.reftime]
        root[PRIOR_COUNT] = str(len(self.prior))
        after_count = len(self.prereqs) + len(self.intermediates) + \
            len(self.finals) + len(self.unused)
        root[AFTER_COUNT] = str(after_count)
        root[DB] = collections.OrderedDict()
        root[DB][PREREQS] = self.prereqs
        root[DB][INTERMEDIATES] = self.intermediates
        root[DB][FINALS] = self.finals
        root[DB][UNUSED] = self.unused

        return root


def cfglog(bump):
    """Configure logging."""
    logging.basicConfig(
        format=PROG + ': %(levelname)s: %(message)s',
        level=max(logging.DEBUG, logging.WARNING - (logging.DEBUG * bump)))


def nfs_flush(priors, host=None):
    """Do whatever it takes to force NFS flushing of metadata."""
    apaths = sorted([os.path.abspath(p) for p in priors if priors[p][2]])
    if host and apaths:
        oldest = int(min((priors[k][1] for k in priors)))
        cmd = ['ssh', host, '--', 'xargs', 'touch', '-a', '-t']
        cmd.append(time.strftime('%Y%m%d%H%M', time.localtime(oldest - DELTA)))
        if len(apaths) > 1:
            logging.info('flushing %d files with "%s"',
                         len(apaths), ' '.join(cmd))
        cmd.insert(1, '-oLogLevel=error')
        proc = subprocess.Popen(cmd, stdin=subprocess.PIPE, encoding='utf-8')
        proc.stdin.write('\n'.join(apaths) + '\n')
        proc.stdin.close()
        if proc.wait():
            sys.exit(2)
    else:
        for path in sorted(priors):
            with open(path) as f:
                fcntl.lockf(f.fileno(), fcntl.LOCK_SH, 1, 0, 0)
                fcntl.lockf(f.fileno(), fcntl.LOCK_UN, 1, 0, 0)


def my_json_dump(data, open_file, current=0, indent=2):
    """Perform json.dump but display each list as a single line."""
    # Unfortunately json.dump() provides very little formatting control,
    # so we'll just re-implement what we need.
    # We assume starting indentation (if needed) has already occurred.
    if isinstance(data, dict) and data:
        open_file.write('{\n')
        new_indent = current + indent
        indent_text = new_indent * ' '
        first = True
        for key in data:
            if not first:  # Add separator (comma) and new line
                open_file.write(',\n')
            open_file.write(indent_text)
            open_file.write(json.dumps(key))
            open_file.write(': ')
            my_json_dump(data[key], open_file, new_indent, indent)
            first = False
        open_file.write('\n')  # End last line
        open_file.write(current * ' ')
        open_file.write('}')
    # Lists don't include long dictionaries, so re-implementation not needed.
    # Instead, everything *but* hashes are forced onto a single line.
    else:
        json.dump(data, open_file)


def generate_json(data, filename):
    """Generate and store JSON data with control over formatting."""
    mkdir_p(os.path.dirname(filename))
    with open(filename, 'w') as f:
        # We don't use "json.dump(data, f, indent=2)" because that produces
        # many multi-line lists.
        my_json_dump(data, f)
        f.write('\n')


def custom_results(custom_list):
    """Convert custom_list, a list of key=value pairs, into a dictionary."""
    result = {}
    if custom_list:
        for entry in custom_list:
            if '=' in entry:
                key, value = entry.split('=', maxsplit=1)
                result[key] = value
            else:  # If no value is given, use true as the value.
                result[entry] = True
    return result


def main():
    """Entry point for standalone use."""
    parser = argparse.ArgumentParser(
        epilog=__doc__.strip(),
        formatter_class=argparse.RawDescriptionHelpFormatter)
    parser.add_argument(
        '-A', '--all-involved', action='store_true',
        help="list all involved files")
    parser.add_argument(
        '-c', '--cmd',
        help="run and audit the specified command line")
    parser.add_argument(
        '-e', '--errexit', action='store_true',
        help="run CMD in -e mode")
    parser.add_argument(
        '--flush-host',
        help="a second host from which to force client flushes")
    parser.add_argument(
        '-k', '--keep-going', action='store_true',
        help="continue even if atimes aren't updated")
    parser.add_argument(
        '-F', '--final-targets', action='store_true',
        help="list final target files")
    parser.add_argument(
        '-I', '--intermediates', action='store_true',
        help="list intermediate target files")
    parser.add_argument(
        '-P', '--prerequisites', action='store_true',
        help="list prerequisite files")
    parser.add_argument(
        '-T', '--targets', action='store_true',
        help="list all target files (intermediate and final")
    parser.add_argument(
        '-U', '--unused', action='store_true',
        help="list files present but unused")
    parser.add_argument(
        '-V', '--verbosity', action='count', default=0,
        help="bump verbosity level")
    parser.add_argument(
        '-W', '--watch', action='append', default=[os.curdir],
        metavar='DIR',
        help="audit activity within DIRs (default=%(default)s)")
    parser.add_argument(
        '-j', '--json',  # default='%s.json' % PROG
        metavar='FILE',
        help="save audit data in JSON format to FILE")
    parser.add_argument(
        '-d', '--depsfile',
        metavar='FILE',
        help="save audit data in makefile format to FILE")
    parser.add_argument(
        '--shell',
        metavar='SHELL_COMMAND',
        help="Name of shell to run (default=SHELL else /bin/sh)")
    parser.add_argument(
        '--shellflags',
        metavar='SHELL_FLAGS',
        help="Space-separated set of flags to pass to shell (default=-c)")
    parser.add_argument(
        '--multiline', action='store_true',
        help="Separately run each line in the command")
    parser.add_argument(
        '--custom', action='append',
        metavar='KEY_VALUE_PAIR',
        help="Include custom key=value pair in JSON results")

    if '--' in sys.argv or '-c' in sys.argv or '--cmd' in sys.argv:
        if '--' in sys.argv:
            opts, cmd = parser.parse_known_args()
            cfglog(opts.verbosity)
            if '--' in cmd:
                cmd.remove('--')
            assert not (opts.cmd or opts.errexit)
            sys.stderr.write('+ %s\n' % ' '.join(cmd))
        else:
            opts = parser.parse_args()
            cfglog(opts.verbosity)
            if opts.shell:
                cmd = [opts.shell]
            else:
                cmd = [os.getenv('SHELL', '/bin/sh')]
            if opts.errexit:
                cmd.append('-e')
            if opts.verbosity:
                cmd.append('-x')
            if opts.shellflags:
                cmd += opts.shellflags.split()
            else:
                cmd.append('-c')
            if opts.multiline:
                result = [cmd + [line] for line in opts.cmd.splitlines()]
                cmd = result
            else:
                cmd.append(opts.cmd)
        if not opts.json and not opts.depsfile:  # Implement default save.
            opts.json = '%s.json' % PROG
        wdirs = []
        for word in opts.watch:
            wdirs.extend(word.split(','))
        exclude_set = set()
        if opts.json:
            exclude_set |= {opts.json}
        if opts.depsfile:
            exclude_set |= {opts.depsfile}
        audit = PMAudit(wdirs, exclude=exclude_set)
        audit.start(flush_host=opts.flush_host, keep_going=opts.keep_going)
        if cmd and cmd[0] and isinstance(cmd[0], list):
            for cmd_line in cmd: # Execute each line in sequence.
                rc = subprocess.call(cmd_line)
                # Stop if we get an error.  Note that "make" does this *even*
                # when -k/--keep-going mode is enabled.
                if rc != 0:
                    break
        else:
            rc = subprocess.call(cmd)
        adb = audit.finish(cmd=opts.cmd or ' '.join(cmd))
        adb['CUSTOM'] = custom_results(opts.custom)
        if opts.depsfile:
            prqs = adb[DB][PREREQS]
            # *Always* create depsfile, even if prqs is empty.
            mkdir_p(os.path.dirname(opts.depsfile))
            with open(opts.depsfile, 'w') as f:
                f.write(os.path.splitext(opts.depsfile)[0] + ': \\\n')
                for i, prq in enumerate(prqs):
                    eol = ' \\\n' if i < len(prqs) - 1 else '\n'
                    f.write('  %s%s' % (prq, eol))
                for prq in prqs:
                    f.write('\n%s:\n' % prq)
        if opts.json:
            generate_json(adb, opts.json)
        sys.exit(2 if rc else 0)

    # Don't analyze, e.g., report from an existing audit log.
    parser.add_argument(
        'dbfile', default='%s.json' % PROG, nargs='?',
        metavar='FILE',
        help="query audit data from FILE (default=%(default)s)")
    opts = parser.parse_args()
    cfglog(opts.verbosity)

    with open(opts.dbfile, 'r') as f:
        root = json.load(f)
    db = root[DB]

    results = set()

    if opts.all_involved:
        opts.prerequisites = opts.intermediates = opts.final_targets = True
    elif opts.targets:
        opts.intermediates = opts.final_targets = True

    if opts.intermediates:
        results.update(db[INTERMEDIATES].keys())
    if opts.prerequisites:
        results.update(db[PREREQS].keys())
    if opts.final_targets:
        results.update(db[FINALS].keys())
    if opts.unused:
        results.update(db[UNUSED].keys())

    for path in sorted(results):
        sys.stdout.write(path + '\n')

    sys.stdout.flush()


if __name__ == '__main__':
    try:
        main()
    except IOError as e:
        # Workaround for an interpreter bug triggered by SIGPIPE.
        # See http://code.activestate.com/lists/python-tutor/88460/
        if 'Broken pipe' not in e.strerror:
            raise

# vim: filetype=python:et:ts=8:sw=4:tw=80
